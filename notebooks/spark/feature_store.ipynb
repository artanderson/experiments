{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Store with Aerospike\n",
    "This notebook shows how Aerospike can be used as a feature store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates in Python how Aerospike can be used as an online feature store. In the first part, it explains and implements the key objects, data model, and operations. In the second part, it uses them in an end-to-end ML prediction scenraio from feature engineering, model training, to model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This tutorial assumes familiarity with the following topics:\n",
    "\n",
    "- Aerospike Notebooks - Readme and Tips\n",
    "- Hello World\n",
    "- PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure Database Is Running\n",
    "This notebook requires that Aerospike datbase is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aerospike database is running!\r\n"
     ]
    }
   ],
   "source": [
    "!asd >& /dev/null\n",
    "!pgrep -x asd >/dev/null && echo \"Aerospike database is running!\" || echo \"**Aerospike database is not running!**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Paths and Env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where spark notebook requisites are installed\n",
    "#SPARK_NB_DIR = '/home/jovyan/notebooks/spark'\n",
    "SPARK_NB_DIR = '/opt/spark-nb'\n",
    "SPARK_HOME = SPARK_NB_DIR + '/spark-3.0.0-bin-hadoop3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IP Address or DNS name for one host in your Aerospike cluster\n",
    "AS_HOST =\"localhost\"\n",
    "# Name of one of your namespaces. Type 'show namespaces' at the aql prompt if you are not sure\n",
    "AS_NAMESPACE = \"test\" \n",
    "AEROSPIKE_SPARK_JAR_VERSION=\"3.1.0\"\n",
    "AS_PORT = 3000 # Usually 3000, but change here if not\n",
    "AS_CONNECTION_STRING = AS_HOST + \":\"+ str(AS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we locate the Spark installation - this will be found using the SPARK_HOME environment variable that you will have set \n",
    "import findspark\n",
    "findspark.init(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aerospike Spark Connector related settings\n",
    "import os \n",
    "AEROSPIKE_JAR_PATH= \"aerospike-spark-assembly-\"+AEROSPIKE_SPARK_JAR_VERSION+\".jar\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--jars ' + SPARK_NB_DIR + '/' + AEROSPIKE_JAR_PATH + ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Spark Session\n",
    "Please visit [Configuring Aerospike Connect for Spark](https://docs.aerospike.com/docs/connect/processing/spark/configuration.html) for more information about the properties used on this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, IntegerType, MapType, LongType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "conf=sc._conf.setAll([(\"aerospike.namespace\",AS_NAMESPACE),(\"aerospike.seedhost\",AS_CONNECTION_STRING)])\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Shell Commands\n",
    "You may execute shell commands including Aerospike tools like [aql](https://docs.aerospike.com/docs/tools/aql/index.html) and [asadm](https://docs.aerospike.com/docs/tools/asadm/index.html) in the terminal tab throughout this tutorial. Open a terminal tab by selecting File->Open from the notebook menu, and then New->Terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Store: Definition and Simple Implementation\n",
    "In this notebook, we will implement and use the following key objects:\n",
    "- Feature Groups\n",
    "- Features\n",
    "- Feature Values\n",
    "- Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store Objects and Data Model\n",
    "### Features Group\n",
    "One way to organize ML features is by grouping them by the source of data and/or processing pipeline. A Feature Group includes many features, and has the following attributes that are useful during feature exploration:\n",
    "- name: a unique name (a string)\n",
    "- description: the description of the feature group\n",
    "- source: source of fact data \n",
    "\n",
    "A feature group is stored in the set \"fg-metadata\".\n",
    "\n",
    "### Feature\n",
    "A Feature consists of the following attributes:\n",
    "- id: the record user-key consists of the string concatenation of fg_name and feature name in the form: fg_name|name.\n",
    "- fg_name: the name of the feature group the feature belongs to\n",
    "- name: feature name that is unique name within the feature group\n",
    "- description: human readable description for exploration\n",
    "- online: a flag indicating if the feature is available online \n",
    "- other: other attributes can include various stats, performance, and other info.\n",
    "\n",
    "A feature group is stored in the set \"feature-metadata\".\n",
    "\n",
    "### Feature Value\n",
    "Feature values are stored for an entity such as a user, credit card, or sensor, and form input records for the ML model datasets. Features in multiple feature groups may be combined for an entity type and their values are stored with each entity instance. A feature values record for an entity has these attributes:\n",
    "- id: id of the entity instance serves as the record key.\n",
    "- fvalues: a map of feature id -> value. The value is encoded as a string prefixed with its type indcator. For example, \"i12\", \"sHarry Potter\", and \"f1.23\".\n",
    "- other: other attributes can include update timestamp, online flag for ease of replication, etc.\n",
    "\n",
    "Feature values for an entity type are stored in entity-type specific set \"entity-type-fvalues\". For example, feature values for users are stored in user-fvalues and for credit cards are stored in cc-fvalues. \n",
    "    \n",
    "### Dataset\n",
    "Datasets hold a selected subset of feature values and entity instances. Only the metadata information is stored in Aerospike while the actual materialization is stored in another store (such as a file system). A dataset record has following attributes.\n",
    "- name: name of the data set, serves as the key for the record\n",
    "- description: description of the dataset\n",
    "- location: location and store details\n",
    "- features: list of features\n",
    "- query: query details to enumerate the entity instances in the dataset.\n",
    "\n",
    "Datasets are stored in the set \"dataset-metadata\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store Operations\n",
    "The following operations are implemented for the different use scenarios:\n",
    "- Feature Engineering\n",
    "    - Feature Group\n",
    "        - create (save)\n",
    "    - Feature\n",
    "        - create (save)\n",
    "    - Feature Values\n",
    "        - create or update (save) individually or from a dataframe\n",
    "\n",
    "- Model Training\n",
    "    - Feature Group\n",
    "        - load (get)\n",
    "    - Feature\n",
    "        - query by group and/or other attribute (query by predicate)\n",
    "    - Feature Values\n",
    "        - query by predicate to get a dataframe\n",
    "    - Dataset\n",
    "        - create (save metadata)\n",
    "\n",
    "- Model Serving\n",
    "    - Feature Values\n",
    "        - get specific feature values for an entity instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store Implementation\n",
    "The following code is a simple implementation of the above operations. These operations will be illustrated in the next section with an end-to-end example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Group Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGroup:\n",
    "    schema = StructType([StructField(\"name\", StringType(), False),\n",
    "                         StructField(\"description\", StringType(), True),\n",
    "                         StructField(\"source\", StringType(), True)])\n",
    "    \n",
    "    def __init__(self, name, description, source):\n",
    "            self.name = name\n",
    "            self.description = description\n",
    "            self.source = source\n",
    "            return\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        inputBuf = [(self.name, self.description, self.source)]\n",
    "        inputRDD = spark.sparkContext.parallelize(inputBuf)            \n",
    "        inputDF = spark.createDataFrame(inputRDD, FeatureGroup.schema)\n",
    "        #Write the data frame to Aerospike, the name field is used as the primary key\n",
    "        inputDF.write \\\n",
    "            .mode('overwrite') \\\n",
    "            .format(\"aerospike\")  \\\n",
    "            .option(\"aerospike.writeset\", \"fg_metadata\")\\\n",
    "            .option(\"aerospike.updateByKey\", \"name\") \\\n",
    "            .save()\n",
    "        return \n",
    "\n",
    "    def load(name):\n",
    "        fg = None\n",
    "        fgdf = spark.read \\\n",
    "            .format(\"aerospike\") \\\n",
    "            .option(\"aerospike.set\", \"fg_metadata\") \\\n",
    "            .schema(FeatureGroup.schema) \\\n",
    "            .load().where(\"name = \\\"\" + name + \"\\\"\")  \n",
    "        if fgdf.count() > 0:\n",
    "            #fgdf = fgdf.select('name', 'description', 'source')\n",
    "            fgtuple = fgdf.collect()[0]\n",
    "            fg = FeatureGroup(*fgtuple)       \n",
    "        return fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.FeatureGroup'>: {'name': 'fg_name1', 'description': 'fg_desc1', 'source': 'fg_source1'}\n"
     ]
    }
   ],
   "source": [
    "#test feature group \n",
    "fg1 = FeatureGroup(\"fg_name1\", \"fg_desc1\", \"fg_source1\")\n",
    "fg1.save()\n",
    "# test read\n",
    "fg2 = FeatureGroup.load(\"fg_name1\")\n",
    "print(fg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    schema = StructType([StructField(\"fid\", StringType(), False),\n",
    "                           StructField(\"fg_name\", StringType(), False),\n",
    "                           StructField(\"name\", StringType(), False),\n",
    "                           StructField(\"description\", StringType(), True),\n",
    "                           StructField(\"online_flag\", IntegerType(), True)])\n",
    "\n",
    "    def __init__(self, fg_name, name, description, online_flag):\n",
    "        self.fid = fg_name + '|' + name\n",
    "        self.fg_name = fg_name\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.online_flag = online_flag\n",
    "        return\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "\n",
    "    def tuple(self):\n",
    "        return (self.fg_name, self.name, self.f_description, self.online_flag)\n",
    "    \n",
    "    def save(self):\n",
    "        inputBuf = [(self.fid, self.fg_name, self.name, self.description, self.online_flag)]\n",
    "        inputRDD = spark.sparkContext.parallelize(inputBuf)            \n",
    "        inputDF = spark.createDataFrame(inputRDD, Feature.schema)\n",
    "        #Write the data frame to Aerospike, the name field is used as the primary key\n",
    "        inputDF.write \\\n",
    "            .mode('overwrite') \\\n",
    "            .format(\"aerospike\")  \\\n",
    "            .option(\"aerospike.writeset\", \"feature_metadata\")\\\n",
    "            .option(\"aerospike.updateByKey\", \"fid\") \\\n",
    "            .save()\n",
    "        return \n",
    "\n",
    "    def load(fg_name, name):\n",
    "        f = None\n",
    "        f_df = spark.read \\\n",
    "            .format(\"aerospike\") \\\n",
    "            .schema(Feature.schema) \\\n",
    "            .option(\"aerospike.set\", \"feature_metadata\") \\\n",
    "            .load().where(\"fid = \\\"\" + fg_name+'|'+name + \"\\\"\")  \n",
    "        #f_df.show();\n",
    "        if f_df.count() > 0:\n",
    "            #f_df = f_df.select('fg_name', 'name', 'description', 'online_flag')\n",
    "            f_tuple = f_df.collect()[0]\n",
    "            f = Feature(*f_tuple[1:])       \n",
    "        return f\n",
    "    \n",
    "    def queryByGroup(fg_name):\n",
    "        features = []\n",
    "        f_df = spark.read \\\n",
    "            .format(\"aerospike\") \\\n",
    "            .schema(Feature.schema) \\\n",
    "            .option(\"aerospike.set\", \"feature_metadata\") \\\n",
    "            .load().where(\"fg_name = \\\"\" + fg_name + \"\\\"\")  \n",
    "        #f_df.show()\n",
    "        #f_df = f_df.select('fg_name', 'name', 'description', 'online_flag')\n",
    "        for i in range(f_df.count()):\n",
    "            f_tuple = f_df.collect()[i]\n",
    "            features.append(Feature(*f_tuple[1:]))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Feature'>: {'fid': 'fg_name1|f_name1', 'fg_name': 'fg_name1', 'name': 'f_name1', 'description': 'f_desc1', 'online_flag': 1}\n",
      "Features in feature group fg_name1:\n",
      "<class '__main__.Feature'>: {'fid': 'fg_name1|f_name2', 'fg_name': 'fg_name1', 'name': 'f_name2', 'description': 'f_desc2', 'online_flag': 1}\n",
      "<class '__main__.Feature'>: {'fid': 'fg_name1|f_name1', 'fg_name': 'fg_name1', 'name': 'f_name1', 'description': 'f_desc1', 'online_flag': 1}\n"
     ]
    }
   ],
   "source": [
    "# test Feature\n",
    "#test save and load\n",
    "feature1 = Feature(\"fg_name1\", \"f_name1\", \"f_desc1\", 1)\n",
    "feature1.save()\n",
    "f1 = Feature.load(\"fg_name1\", \"f_name1\")\n",
    "print(f1)\n",
    "# test query\n",
    "feature2 = Feature(\"fg_name1\", \"f_name2\", \"f_desc2\", 1)\n",
    "feature2.save()\n",
    "features = Feature.queryByGroup('fg_name1')\n",
    "print(\"Features in feature group fg_name1:\")\n",
    "for f in features:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Values Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureValues:\n",
    "    schema = StructType([StructField(\"eid\", StringType(), False),\n",
    "                         StructField(\"fvalues\", MapType(StringType(), StringType()), True)])\n",
    "\n",
    "    def __init__(self, etype, eid, fvalues):\n",
    "        self.etype = etype\n",
    "        self.eid = eid\n",
    "        self.fvalues = fvalues\n",
    "        return\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "        \n",
    "    def valToDB(value):\n",
    "        conv = {str:'s', int:'i', float:'f'}\n",
    "        return conv[type(value)]+str(value)\n",
    "    \n",
    "    def valFromDB(vstr):\n",
    "        t = vstr[0]\n",
    "        v = vstr[1:]\n",
    "        return float(v) if t == 'f' else (int(v) if t == 'i' else v)\n",
    "            \n",
    "    def fvalsToDB(fvals):\n",
    "        dbrep = {}\n",
    "        for f,v in fvals.items():\n",
    "            dbrep[f] = FeatureValues.valToDB(v)\n",
    "        return dbrep\n",
    "    \n",
    "    def fvalsFromDB(dbrep):\n",
    "        fvals = {}\n",
    "        for f,v in dbrep.items():\n",
    "            fvals[f] = FeatureValues.valFromDB(v)\n",
    "        return fvals       \n",
    "        \n",
    "    def save(self):\n",
    "        inputBuf = [(self.eid, FeatureValues.fvalsToDB(self.fvalues))]\n",
    "        inputRDD = spark.sparkContext.parallelize(inputBuf)            \n",
    "        inputDF = spark.createDataFrame(inputRDD, FeatureValues.schema)\n",
    "        #Write the data frame to Aerospike, the fg_name field is used as the primary key\n",
    "        inputDF.write \\\n",
    "            .mode('overwrite') \\\n",
    "            .format(\"aerospike\")  \\\n",
    "            .option(\"aerospike.writeset\", self.etype+'-fvalues')\\\n",
    "            .option(\"aerospike.updateByKey\", \"eid\") \\\n",
    "            .save()\n",
    "        return \n",
    "\n",
    "    def load(etype, eid):\n",
    "        fv = None\n",
    "        fv_df = spark.read \\\n",
    "            .format(\"aerospike\") \\\n",
    "            .schema(FeatureValues.schema) \\\n",
    "            .option(\"aerospike.set\", etype+'-fvalues') \\\n",
    "            .option(\"aerospike.keyType\", \"string\") \\\n",
    "            .load().where(\"eid = \\\"\" + eid + \"\\\"\")  \n",
    "        if fv_df.count() > 0:\n",
    "            #fv_df = fv_df.select('eid', 'fvalues')           \n",
    "            fv_tuple = fv_df.collect()[0]\n",
    "            fv = FeatureValues(etype, fv_tuple[0], FeatureValues.fvalsFromDB(fv_tuple[1]))       \n",
    "        return fv\n",
    "    \n",
    "    def saveDF(df, etype, fg_name, eid_column): # save a dataframe\n",
    "    # df: dataframe consisting of feature values\n",
    "    # eid_column: column name that holds the eid value\n",
    "    # fg: feature group of the features\n",
    "        list_entities = map(lambda row: row.asDict(), df.collect())\n",
    "        #print(\"list_entities: \", list_entities)\n",
    "        inputBuf = [(entity[eid_column], {fg_name+'|'+k : FeatureValues.valToDB(v) \\\n",
    "                                          for k,v in entity.items() if k != eid_column}) for entity in list_entities]\n",
    "        #print(\"inputBuf: \", inputBuf)\n",
    "        inputRDD = spark.sparkContext.parallelize(inputBuf)            \n",
    "        inputDF = spark.createDataFrame(inputRDD, FeatureValues.schema)\n",
    "        #inputDF.show()\n",
    "        #Write the data frame to Aerospike, the name field is used as the primary key\n",
    "        inputDF.write \\\n",
    "            .mode('overwrite') \\\n",
    "            .format(\"aerospike\")  \\\n",
    "            .option(\"aerospike.writeset\", etype+'-fvalues')\\\n",
    "            .option(\"aerospike.updateByKey\", \"eid\") \\\n",
    "            .save()\n",
    "        return \n",
    "        \n",
    "    \n",
    "    def query(etype, predicate): #returns a dataframe\n",
    "        def add_map_element(m, k, v): #lambda function\n",
    "            m[k] = v\n",
    "            return m\n",
    "        \n",
    "        fv_df = spark.read \\\n",
    "            .format(\"aerospike\") \\\n",
    "            .schema(FeatureValues.schema) \\\n",
    "            .option(\"aerospike.set\", etype+'-fvalues') \\\n",
    "            .option(\"aerospike.keyType\", \"string\") \\\n",
    "            .load().where(predicate)\n",
    "        #fv_df = fv_df.select('eid', 'fvalues')\n",
    "        #fv_df.show(5)\n",
    "        list_entities = map(lambda row: add_map_element(row[1], '|ID', row[0]), fv_df.collect())\n",
    "        buf = [{k.partition('|')[2] : FeatureValues.valFromDB(v) for k,v in entity.items()} for entity in list_entities]\n",
    "        rdd = spark.sparkContext.parallelize(buf)            \n",
    "        fv_df2 = spark.createDataFrame(rdd) \n",
    "        return fv_df2\n",
    "        \n",
    "    def get_feature_vector(etype, eid, feature_list): # elements in feature_list are in \"fg_name|name\" form\n",
    "        # the implementation is shown in Model Serving section with no Spark dependency\n",
    "        # using Aerospike client\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.FeatureValues'>: {'etype': 'entity_type1', 'eid': 'eid1', 'fvalues': {'fg_name1|f_name1': 1, 'fg_name1|f_name2': 2.0, 'fg_name1|f_name3': 'three'}}\n",
      "Instances of entity type entity_type1 with eid ending in 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/notebooks/spark/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/session.py:398: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+-------+\n",
      "| ID|f_name1|f_name2|f_name3|\n",
      "+---+-------+-------+-------+\n",
      "|id1|      1|    2.0|  three|\n",
      "+---+-------+-------+-------+\n",
      "\n",
      "Specified instances of entity type entity_type2:\n",
      "+---+-------+-------+-------+\n",
      "| ID|f_name1|f_name2|f_name3|\n",
      "+---+-------+-------+-------+\n",
      "|id2|     10|   20.0| thirty|\n",
      "+---+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test Feature Values\n",
    "# test save and load\n",
    "fvals1 = {'fg_name1|f_name1' : 1, 'fg_name1|f_name2' : 2.0, 'fg_name1|f_name3' : 'three'}\n",
    "feature_values1 = FeatureValues('entity_type1', 'eid1', fvals1)\n",
    "feature_values1.save();\n",
    "fv1 = FeatureValues.load('entity_type1', 'eid1')\n",
    "print(fv1)\n",
    "# test query\n",
    "fvals2 = {'fg_name1|f_name1' : 10, 'fg_name1|f_name2' : 20.0, 'fg_name1|f_name3' : 'thirty'}\n",
    "feature_values2 = FeatureValues('entity_type2', 'eid2', fvals2)\n",
    "feature_values2.save();\n",
    "print(\"Instances of entity type entity_type1 with eid ending in 1:\")\n",
    "instances = FeatureValues.query('entity_type1', 'eid like \"%1\"')\n",
    "instances.show()\n",
    "print(\"Specified instances of entity type entity_type2:\")\n",
    "instances = FeatureValues.query('entity_type2', 'eid in (\"eid2\")')\n",
    "instances.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+\n",
      "|   ID|feature_1|feature_2|\n",
      "+-----+---------+---------+\n",
      "|eid_1|        1|     12.4|\n",
      "|eid_2|        2|     30.1|\n",
      "|eid_3|        3|   100.01|\n",
      "+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# test save dataframe\n",
    "data = [{\"ID\": 'eid_1', \"feature_1\": 1, \"feature_2\": 12.40},\n",
    "        {\"ID\": 'eid_2', \"feature_1\": 2, \"feature_2\": 30.10},\n",
    "        {\"ID\": 'eid_3', \"feature_1\": 3, \"feature_2\": 100.01}\n",
    "        ]\n",
    "\n",
    "# Create data frame\n",
    "# create a new ID column if the id column needs to be preserved as a feature \n",
    "# using df.withColumn(\"ID\", df[\"id-feature\"])\n",
    "df = spark.createDataFrame([Row(**i) for i in data])\n",
    "df.show()\n",
    "FeatureValues.saveDF(df, \"etype_1\", \"fg_1\", \"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+\n",
      "|  ID|feature_1|feature_2|\n",
      "+----+---------+---------+\n",
      "|id_3|        3|   100.01|\n",
      "|id_1|        1|     12.4|\n",
      "+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test query\n",
    "queryDF = FeatureValues.query('etype_1', 'eid in (\"eid_1\", \"eid_3\")')\n",
    "queryDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Feature Store\n",
    "In this section, we show how the feature store objects and operations can be leveraged through the various phases of the ML flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created\n"
     ]
    }
   ],
   "source": [
    "# We create age vs salary data, using three different Gaussian distributions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Make sure we get the same results every time this workbook is run\n",
    "# Otherwise we are occasionally exposed to results not working out as expected\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Create covariance matrix from std devs + correlation\n",
    "def covariance_matrix(std_dev_1,std_dev_2,correlation):\n",
    "    return [[std_dev_1 ** 2, correlation * std_dev_1 * std_dev_2], \n",
    "           [correlation * std_dev_1 * std_dev_2, std_dev_2 ** 2]]\n",
    "\n",
    "# Return a bivariate sample given means/std dev/correlation\n",
    "def age_salary_sample(distribution_params,sample_size):\n",
    "    mean = [distribution_params[\"age_mean\"], distribution_params[\"salary_mean\"]]\n",
    "    cov = covariance_matrix(distribution_params[\"age_std_dev\"],distribution_params[\"salary_std_dev\"],\n",
    "                            distribution_params[\"age_salary_correlation\"])\n",
    "    return np.random.multivariate_normal(mean, cov, sample_size).T\n",
    "\n",
    "# Define the characteristics of our age/salary distribution\n",
    "age_salary_distribution_1 = {\"age_mean\":25,\"salary_mean\":50000,\n",
    "                             \"age_std_dev\":1,\"salary_std_dev\":5000,\"age_salary_correlation\":0.3}\n",
    "\n",
    "age_salary_distribution_2 = {\"age_mean\":45,\"salary_mean\":80000,\n",
    "                             \"age_std_dev\":4,\"salary_std_dev\":8000,\"age_salary_correlation\":0.7}\n",
    "\n",
    "age_salary_distribution_3 = {\"age_mean\":35,\"salary_mean\":70000,\n",
    "                             \"age_std_dev\":2,\"salary_std_dev\":9000,\"age_salary_correlation\":0.1}\n",
    "\n",
    "distribution_data = [age_salary_distribution_1,age_salary_distribution_2,age_salary_distribution_3]\n",
    "\n",
    "# Sample age/salary data for each distributions\n",
    "sample_size_1 = 100;\n",
    "sample_size_2 = 120;\n",
    "sample_size_3 = 80;\n",
    "sample_sizes = [sample_size_1,sample_size_2,sample_size_3]\n",
    "group_1_ages,group_1_salaries = age_salary_sample(age_salary_distribution_1,sample_size=sample_size_1)\n",
    "group_2_ages,group_2_salaries = age_salary_sample(age_salary_distribution_2,sample_size=sample_size_2)\n",
    "group_3_ages,group_3_salaries = age_salary_sample(age_salary_distribution_3,sample_size=sample_size_3)\n",
    "\n",
    "ages=np.concatenate([group_1_ages,group_2_ages,group_3_ages])\n",
    "salaries=np.concatenate([group_1_salaries,group_2_salaries,group_3_salaries])\n",
    "\n",
    "print(\"Data created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display simulated age/salary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the sample data\n",
    "group_1_colour, group_2_colour, group_3_colour ='red','blue', 'pink'\n",
    "plt.xlabel('Age',fontsize=10)\n",
    "plt.ylabel(\"Salary\",fontsize=10) \n",
    "\n",
    "plt.scatter(group_1_ages,group_1_salaries,c=group_1_colour,label=\"Group 1\")\n",
    "plt.scatter(group_2_ages,group_2_salaries,c=group_2_colour,label=\"Group 2\")\n",
    "plt.scatter(group_3_ages,group_3_salaries,c=group_3_colour,label=\"Group 3\")\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data (Features) to Aerospike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the above records into a Data Frame\n",
    "# First of all, create an array of arrays\n",
    "inputBuf = []\n",
    "\n",
    "for  i in range(0, len(ages)) :\n",
    "     id = i + 1 # Avoid counting from zero\n",
    "     name = \"Individual: {:03d}\".format(id)\n",
    "     # Note we need to make sure values are typed correctly\n",
    "     # salary will have type numpy.float64 - if it is not cast as below, an error will be thrown\n",
    "     age = float(ages[i])\n",
    "     salary = int(salaries[i])\n",
    "     inputBuf.append((id, name,age,salary))\n",
    "\n",
    "# Convert to an RDD \n",
    "inputRDD = spark.sparkContext.parallelize(inputBuf)\n",
    "       \n",
    "# Convert to a data frame using a schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"salary\",IntegerType(), True)\n",
    "])\n",
    "\n",
    "inputDF=spark.createDataFrame(inputRDD,schema)\n",
    "\n",
    "''' USE THE FEATURE STORE APIS INSTEAD - BELOW\n",
    "#Write the data frame to Aerospike, the id field is used as the primary key\n",
    "inputDF \\\n",
    ".write \\\n",
    ".mode('overwrite') \\\n",
    ".format(\"aerospike\")  \\\n",
    ".option(\"aerospike.set\", \"salary_data\")\\\n",
    ".option(\"aerospike.updateByKey\", \"id\") \\\n",
    ".save()\n",
    "'''\n",
    "# Create a feature group\n",
    "FG_NAME = 'age_sal'\n",
    "FG_DESCRIPTION = 'Age salary data'\n",
    "FG_SOURCE = 'generated sample data'\n",
    "fg = FeatureGroup(FG_NAME, FG_DESCRIPTION, FG_SOURCE)\n",
    "fg.save()\n",
    "\n",
    "# create features metadata\n",
    "FEATURE_NAME = 'name'\n",
    "f_name = Feature(FG_NAME, FEATURE_NAME, \"name of the person\", 1)\n",
    "f_name.save()\n",
    "\n",
    "FEATURE_AGE = 'age'\n",
    "f_age = Feature(FG_NAME, FEATURE_AGE, \"age of the person\", 1)\n",
    "f_age.save()\n",
    "\n",
    "FEATURE_SALARY = 'salary'\n",
    "f_sal = Feature(FG_NAME, FEATURE_SALARY, \"salary of the person\", 1)\n",
    "f_sal.save()\n",
    "\n",
    "# save the feature values\n",
    "ENTITY_TYPE = 'user'\n",
    "ID_COLUMN = \"id\"\n",
    "FeatureValues.saveDF(inputDF, ENTITY_TYPE, FG_NAME, ID_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the data through aql with the following command:\n",
    "'''\n",
    "aql -c \"select * from test.user-fvalues\"\n",
    "'''\n",
    "The output will look like:\n",
    "'''\n",
    "select * from test.user-fvalues\n",
    "+-------+------------------------------------------------------------------------------------------------------------------------+\n",
    "| eid   | fvalues      |\n",
    "+-------+------------------------------------------------------------------------------------------------------------------------+\n",
    "| \"62\"  | KEY_ORDERED_MAP('{\"age_sal|age\":\"f25.77585254835976\", \"age_sal|salary\":\"i51768\", \"age_sal|name\":\"sIndividual: 062\"}')  |\n",
    "| \"281\" | KEY_ORDERED_MAP('{\"age_sal|age\":\"f38.08819977056483\", \"age_sal|salary\":\"i75612\", \"age_sal|name\":\"sIndividual: 281\"}')  |\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DataFrame using user specified schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+---------------+------+\n",
      "| ID|               age|           name|salary|\n",
      "+---+------------------+---------------+------+\n",
      "|  2| 25.77585254835976|Individual: 062| 51768|\n",
      "| 81| 38.08819977056483|Individual: 281| 75612|\n",
      "| 26| 45.82761289603616|Individual: 126| 91152|\n",
      "| 42| 42.56064799325679|Individual: 142| 80357|\n",
      "| 11|45.089995066293625|Individual: 111| 76434|\n",
      "+---+------------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/notebooks/spark/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/session.py:398: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "# If we explicitly set the schema, using the previously created schema object\n",
    "# we effectively type the rows in the Data Frame\n",
    "\n",
    "''' USE DATA STORE API TO GET THE DATAFRAME\n",
    "loadedDFWithSchema=spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".schema(schema) \\\n",
    ".option(\"aerospike.set\", \"salary_data\").load()\n",
    "'''\n",
    "\n",
    "loadedDFWithSchema = FeatureValues.query(ENTITY_TYPE, \"eid > '0'\")\n",
    "loadedDFWithSchema.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from Aerospike DB\n",
    "\n",
    "- Sample specified number of records from Aerospike to considerably reduce data movement between Aerospike and the Spark clusters. Depending on the aerospike.partition.factor setting, you may get more records than desired. Please use this property in conjunction with Spark `limit()` function to get the specified number of records. The sample read is not randomized, so sample more than you need and use the Spark `sample()` function to randomize if you see fit. You can use it in conjunction with `aerospike.recordspersecond` to control the load on the Aerospike server while sampling.\n",
    "\n",
    "- For more information, please see [documentation](https://docs.aerospike.com/docs/connect/processing/spark/configuration.html) page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration with Aerospike "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#convert Spark df to pandas df\n",
    "pdf = loadedDFWithSchema.toPandas()\n",
    "\n",
    "# Describe the data\n",
    "\n",
    "pdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram - Age\n",
    "age_min, age_max = int(np.amin(pdf['age'])), math.ceil(np.amax(pdf['age']))\n",
    "age_bucket_size = 5\n",
    "print(age_min,age_max)\n",
    "pdf[['age']].plot(kind='hist',bins=range(age_min,age_max,age_bucket_size),rwidth=0.8)\n",
    "plt.xlabel('Age',fontsize=10)\n",
    "plt.legend(loc=None)\n",
    "plt.show()\n",
    "\n",
    "#Histogram - Salary\n",
    "salary_min, salary_max = int(np.amin(pdf['salary'])), math.ceil(np.amax(pdf['salary']))\n",
    "salary_bucket_size = 5000\n",
    "pdf[['salary']].plot(kind='hist',bins=range(salary_min,salary_max,salary_bucket_size),rwidth=0.8)\n",
    "plt.xlabel('Salary',fontsize=10)\n",
    "plt.legend(loc=None)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap\n",
    "age_bucket_count = math.ceil((age_max - age_min)/age_bucket_size)\n",
    "salary_bucket_count = math.ceil((salary_max - salary_min)/salary_bucket_size)\n",
    "\n",
    "x = [[0 for i in range(salary_bucket_count)] for j in range(age_bucket_count)]\n",
    "for i in range(len(pdf['age'])):\n",
    "    age_bucket = math.floor((pdf['age'][i] - age_min)/age_bucket_size)\n",
    "    salary_bucket = math.floor((pdf['salary'][i] - salary_min)/salary_bucket_size)\n",
    "    x[age_bucket][salary_bucket] += 1\n",
    "\n",
    "plt.title(\"Salary/Age distribution heatmap\")\n",
    "plt.xlabel(\"Salary in '000s\")\n",
    "plt.ylabel(\"Age\")\n",
    "\n",
    "plt.imshow(x, cmap='YlOrRd', interpolation='nearest',extent=[salary_min/1000,salary_max/1000,age_min,age_max],\n",
    "           origin=\"lower\")\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning using Aerospike / Spark\n",
    "\n",
    "In this section we use the data we took from Aerospike and apply a clustering algorithm to it.\n",
    "\n",
    "We assume the data is composed of multiple data sets having a Gaussian multi-variate distribution\n",
    "\n",
    "We don't know how many clusters there are, so we try clustering based on the assumption there are 1 through 20.\n",
    "\n",
    "We compare the quality of the results using the Bayesian Information Criterion - https://en.wikipedia.org/wiki/Bayesian_information_criterion and pick the best.\n",
    "    \n",
    "### Find Optimal Cluster Count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# We take the data we previously \n",
    "ages=pdf['age']\n",
    "salaries=pdf['salary']\n",
    "age_salary_matrix=np.matrix([ages,salaries]).T\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "optimal_cluster_count = 1\n",
    "best_bic_score = GaussianMixture(1).fit(age_salary_matrix).bic(age_salary_matrix)\n",
    "\n",
    "for count in range(1,20):\n",
    "    gm=GaussianMixture(count)\n",
    "    gm.fit(age_salary_matrix)\n",
    "    if gm.bic(age_salary_matrix) < best_bic_score:\n",
    "        best_bic_score = gm.bic(age_salary_matrix)\n",
    "        optimal_cluster_count = count\n",
    "\n",
    "print(\"Optimal cluster count found to be \"+str(optimal_cluster_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate cluster distribution parameters\n",
    "Next we fit our cluster using the optimal cluster count, and print out the discovered means and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(optimal_cluster_count)\n",
    "gm.fit(age_salary_matrix)\n",
    "\n",
    "estimates = []\n",
    "# Index\n",
    "for index in range(0,optimal_cluster_count):\n",
    "    estimated_mean_age = round(gm.means_[index][0],2)\n",
    "    estimated_mean_salary = round(gm.means_[index][1],0)\n",
    "    estimated_age_std_dev = round(math.sqrt(gm.covariances_[index][0][0]),2)\n",
    "    estimated_salary_std_dev = round(math.sqrt(gm.covariances_[index][1][1]),0)\n",
    "    estimated_correlation = round(gm.covariances_[index][0][1] / ( estimated_age_std_dev * estimated_salary_std_dev ),3)\n",
    "    row = [estimated_mean_age,estimated_mean_salary,estimated_age_std_dev,estimated_salary_std_dev,estimated_correlation]\n",
    "    estimates.append(row)\n",
    "    \n",
    "pd.DataFrame(estimates,columns = [\"Est Mean Age\",\"Est Mean Salary\",\"Est Age Std Dev\",\"Est Salary Std Dev\",\"Est Correlation\"])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Distribution Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_data_as_rows = []\n",
    "for distribution in distribution_data:\n",
    "    row = [distribution['age_mean'],distribution['salary_mean'],distribution['age_std_dev'],\n",
    "                             distribution['salary_std_dev'],distribution['age_salary_correlation']]\n",
    "    distribution_data_as_rows.append(row)\n",
    "\n",
    "pd.DataFrame(distribution_data_as_rows,columns = [\"Mean Age\",\"Mean Salary\",\"Age Std Dev\",\"Salary Std Dev\",\"Correlation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the algorithm provides good estimates of the original parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Feature Vector Implementation\n",
    "The Spark environment may not be available for model serving. Here the Python client library is used to get specific features for a specific entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the map items by key list from the map record bin\n",
    "# use: aerospike_helpers.operations.map_operations.map_get_by_key_list(bin_name, key_list, \n",
    "#                                                                return_type, inverted=False, ctx=None)\n",
    "# initialize client, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We generate new age/salary pairs for each of the distributions and look at how accurate the prediction is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_accuracy(model,age_salary_distribution,sample_size):\n",
    "    # Generate new values\n",
    "    new_ages,new_salaries = age_salary_sample(age_salary_distribution,sample_size)\n",
    "    new_age_salary_matrix=np.matrix([new_ages,new_salaries]).T\n",
    "    # Find which cluster the mean would be classified into\n",
    "    mean = np.matrix([age_salary_distribution['age_mean'],age_salary_distribution['salary_mean']])\n",
    "    mean_cluster_index = model.predict(mean)[0]\n",
    "    # How would new samples be classified\n",
    "    classification = model.predict(new_age_salary_matrix)\n",
    "    # How many were classified correctly\n",
    "    correctly_classified = len([ 1 for x in classification if x  == mean_cluster_index])\n",
    "    return correctly_classified / sample_size\n",
    "\n",
    "prediction_accuracy_results = [None for x in range(3)]\n",
    "for index, age_salary_distribution in enumerate(distribution_data):\n",
    "    prediction_accuracy_results[index] = prediction_accuracy(gm,age_salary_distribution,1000)\n",
    "\n",
    "overall_accuracy = sum(prediction_accuracy_results)/ len(prediction_accuracy_results)\n",
    "print(\"Accuracies for each distribution : \",\" ,\".join(map('{:.2%}'.format,prediction_accuracy_results)))\n",
    "print(\"Overall accuracy : \",'{:.2%}'.format(overall_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
