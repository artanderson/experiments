{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Implementing-SQL-Operations:-Aggregates\" data-toc-modified-id=\"Implementing-SQL-Operations:-Aggregates-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Implementing SQL Operations: Aggregates</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Prerequisites\" data-toc-modified-id=\"Prerequisites-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Prerequisites</a></span></li><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Initialization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ensure-database-is-running\" data-toc-modified-id=\"Ensure-database-is-running-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Ensure database is running</a></span></li><li><span><a href=\"#Download-and-install-additional-components.\" data-toc-modified-id=\"Download-and-install-additional-components.-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Download and install additional components.</a></span></li></ul></li><li><span><a href=\"#Connect-to-database-and-populate-test-data\" data-toc-modified-id=\"Connect-to-database-and-populate-test-data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Connect to database and populate test data</a></span></li><li><span><a href=\"#Create-a-secondary-index\" data-toc-modified-id=\"Create-a-secondary-index-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Create a secondary index</a></span></li></ul></li><li><span><a href=\"#Execution-model\" data-toc-modified-id=\"Execution-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Execution model</a></span></li><li><span><a href=\"#Simple-Map-Reduce-Aggregates\" data-toc-modified-id=\"Simple-Map-Reduce-Aggregates-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Simple Map-Reduce Aggregates</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-User-Defined-Function-(UDF)\" data-toc-modified-id=\"Create-User-Defined-Function-(UDF)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create User Defined Function (UDF)</a></span></li><li><span><a href=\"#Register-UDF\" data-toc-modified-id=\"Register-UDF-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Register UDF</a></span></li><li><span><a href=\"#Execute-UDF\" data-toc-modified-id=\"Execute-UDF-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Execute UDF</a></span></li></ul></li><li><span><a href=\"#Using-Aggregate-Operator\" data-toc-modified-id=\"Using-Aggregate-Operator-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Using Aggregate Operator</a></span></li><li><span><a href=\"#Stream-Partitioning-with-GROUP-BY\" data-toc-modified-id=\"Stream-Partitioning-with-GROUP-BY-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Stream Partitioning with GROUP BY</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filtering-Partitions:-HAVING\" data-toc-modified-id=\"Filtering-Partitions:-HAVING-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Filtering Partitions: HAVING</a></span></li><li><span><a href=\"#Sorting-Partitions:-ORDER-BY\" data-toc-modified-id=\"Sorting-Partitions:-ORDER-BY-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Sorting Partitions: ORDER BY</a></span></li></ul></li><li><span><a href=\"#More-Aggregates:-Distinct-and-Top-N\" data-toc-modified-id=\"More-Aggregates:-Distinct-and-Top-N-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>More Aggregates: Distinct and Top N</a></span></li><li><span><a href=\"#Takeaways-and-Conclusion\" data-toc-modified-id=\"Takeaways-and-Conclusion-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Takeaways and Conclusion</a></span></li><li><span><a href=\"#Clean-up\" data-toc-modified-id=\"Clean-up-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Clean up</a></span></li><li><span><a href=\"#Further-Exploration-and-Resources\" data-toc-modified-id=\"Further-Exploration-and-Resources-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Further Exploration and Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#Next-steps\" data-toc-modified-id=\"Next-steps-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Next steps</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing SQL Operations: Aggregates\n",
    "This tutorial describes how to implement SQL aggregate queries in Aerospike.\n",
    "\n",
    "This notebook requires Aerospike datbase running on localhost and that python and the Aerospike python client have been installed (`pip install aerospike`). Visit [Aerospike notebooks repo](https://github.com/aerospike-examples/interactive-notebooks) for additional details and the docker container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we will see how specific aggregate statements in SQL can be implemented in Aerospike. \n",
    "\n",
    "SQL is a widely known data access language. If you have used SQL, the examples in this notebook should provide a pattern for implementing specific SQL aggregate queries. \n",
    "\n",
    "This notebook is the second in the SQL Operations series that consists of the following notebooks:\n",
    "- Implementing SQL Operations: SELECT\n",
    "- Implementing SQL Operations: Aggregates (this notebook)\n",
    "- Implementing SQL Operations: UPDATE, CREATE, and DELETE\n",
    "\n",
    "The specific topics and aggregates statements we discuss include:\n",
    "- Execution model\n",
    "    - Stream processing using UDF functions\n",
    "    - Four type of operators: Filter, Map, Aggregate, and Reduce\n",
    "    - Two phases of reduce: On server nodes and on client \n",
    "- Simple Map-Reduce Aggregations:\n",
    "    - MIN\n",
    "    - MAX\n",
    "    - AVERAGE\n",
    "    - SUM\n",
    "    - COUNT\n",
    "- Using Aggregate Operator\n",
    "    - AVERAGE\n",
    "- Stream Partitioning with GROUP BY\n",
    "    - Filtering partitions: HAVING\n",
    "    - Sorting partitions: ORDER BY\n",
    "- Other aggregates\n",
    "    - DISTINCT\n",
    "    - TOP N\n",
    "\n",
    "The purpose of this notebook is to illustrate Aerospike implementation for specific SQL operations. Check out [Aerospike Presto Connector](https://www.aerospike.com/docs/connect/access/presto/index.html) for ad-hoc SQL access to Aerospike data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This tutorial assumes familiarity with the following topics:\n",
    "- [Hello World](hello_world.ipynb)\n",
    "- [Implementing SQL Operations: SELECT](sql_select.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Ensure database is running\n",
    "This notebook requires that Aerospike datbase is running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T20:48:49.065421Z",
     "start_time": "2020-12-29T20:48:49.060897Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import io.github.spencerpark.ijava.IJava;\n",
    "import io.github.spencerpark.jupyter.kernel.magic.common.Shell;\n",
    "IJava.getKernelInstance().getMagics().registerMagics(Shell.class);\n",
    "%sh asd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Download and install additional components.\n",
    "Install the Java client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T20:48:50.084636Z",
     "start_time": "2020-12-29T20:48:50.080629Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<dependencies>\n",
    "  <dependency>\n",
    "    <groupId>com.aerospike</groupId>\n",
    "    <artifactId>aerospike-client</artifactId>\n",
    "    <version>5.0.0</version>\n",
    "  </dependency>\n",
    "</dependencies>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Connect to database and populate test data\n",
    "The test data has ten records with user-key \"id-1\" through \"id-10\", two integer bins (fields) \"bin1\" and \"bin2\", in the namespace \"test\" and sets \"sql-select-small\"and null, and similarly structured 1000 records in set \"sql-select-large\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T20:48:50.771243Z",
     "start_time": "2020-12-29T20:48:50.767819Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized the client and connected to the cluster.\n",
      "Test data popuated"
     ]
    }
   ],
   "source": [
    "import com.aerospike.client.AerospikeClient;\n",
    "import com.aerospike.client.Bin;\n",
    "import com.aerospike.client.Key;\n",
    "//import com.aerospike.client.policy.ClientPolicy;\n",
    "import com.aerospike.client.policy.WritePolicy;\n",
    "import java.util.Random; \n",
    "\n",
    "String[] groups = {\"A\", \"B\", \"C\", \"D\", \"E\"}; \n",
    "Random rand = new Random(1); \n",
    "\n",
    "AerospikeClient client = new AerospikeClient(\"localhost\", 3000);\n",
    "System.out.println(\"Initialized the client and connected to the cluster.\");\n",
    "\n",
    "String Namespace = \"test\";\n",
    "String SmallSet = \"sql-select-small\";\n",
    "String LargeSet = \"sql-select-large\";\n",
    "String NullSet = \"\";\n",
    "\n",
    "//ClientPolicy policy = new ClientPolicy();\n",
    "WritePolicy wpolicy = new WritePolicy();\n",
    "wpolicy.sendKey = true;\n",
    "for (int i = 1; i <= 10; i++) {\n",
    "    Key key = new Key(Namespace, SmallSet, \"id-\"+i);\n",
    "    Bin bin1 = new Bin(new String(\"bin1\"), i);\n",
    "    Bin bin2 = new Bin(new String(\"bin2\"), 1000+i);\n",
    "    client.put(wpolicy, key, bin1, bin2);\n",
    "}\n",
    "for (int i = 1; i <= 10; i++) {\n",
    "    Key key = new Key(Namespace, NullSet, \"id-\"+i);\n",
    "    Bin bin1 = new Bin(new String(\"bin1\"), i);\n",
    "    Bin bin2 = new Bin(new String(\"bin2\"), 1000+i);\n",
    "    client.put(wpolicy, key, bin1, bin2);\n",
    "}\n",
    "for (int i = 1; i <= 1000; i++) {\n",
    "    Key key = new Key(Namespace, LargeSet, \"id-\"+i);\n",
    "    Bin bin1 = new Bin(new String(\"bin1\"), i);\n",
    "    Bin bin2 = new Bin(new String(\"bin2\"), 1000+i);\n",
    "    Bin bin3 = new Bin(new String(\"bin3\"), groups[rand.nextInt(groups.length)]); \n",
    "    client.put(wpolicy, key, bin1, bin2, bin3);\n",
    "}\n",
    "\n",
    "System.out.format(\"Test data popuated\");;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index test_small_bin1_number_idx on ns=test set=sql-select-small bin=bin1."
     ]
    }
   ],
   "source": [
    "import com.aerospike.client.policy.Policy;\n",
    "import com.aerospike.client.query.IndexType;\n",
    "import com.aerospike.client.task.IndexTask;\n",
    "import com.aerospike.client.AerospikeException;\n",
    "import com.aerospike.client.ResultCode;\n",
    "\n",
    "String IndexName = \"test_small_bin1_number_idx\";\n",
    "\n",
    "Policy policy = new Policy();\n",
    "policy.socketTimeout = 0; // Do not timeout on index create.\n",
    "\n",
    "try {\n",
    "    IndexTask task = client.createIndex(policy, Namespace, SmallSet, IndexName, \n",
    "                                        \"bin1\", IndexType.NUMERIC);\n",
    "    task.waitTillComplete();\n",
    "}\n",
    "catch (AerospikeException ae) {\n",
    "    if (ae.getResultCode() != ResultCode.INDEX_ALREADY_EXISTS) {\n",
    "        throw ae;\n",
    "    }\n",
    "}\n",
    "\n",
    "System.out.format(\"Created index %s on ns=%s set=%s bin=%s.\", \n",
    "                                    IndexName, Namespace, SmallSet, \"bin1\");;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution model\n",
    "Key points\n",
    "- Stream processing using UDF functions. \n",
    "    - UDF stream functions must be used with query operation. A stream function specifies the pipeline of operators for processing all records. \n",
    "- Four type of operators: Filter, Map, Aggregate, and Reduce\n",
    "    - The operators that can be specified in any order fall into 4 categories: filter (obj -> boolean), map (object -> object), aggregate ((state, object) -> new state), and reduce ((value1, value2) -> value).\n",
    "- Two phases of reduce: On server nodes and on client \n",
    "    - The server nodes execute all operators up to and including the first reduce operation in the pipeline. The client processes the node results with the remaining pipeline operators starting with and including the first reduce operation in the pipeline. Thus, the first reduce operation if specified in the pipeline is executed on all server nodes as well as on the client. \n",
    "- Post aggregation processing. \n",
    "    - Post aggregation processing for sorting and filtering must happen on the client side typically with a map operator. \n",
    "    \n",
    "CONFIRM: \n",
    "- Every node processes only up to first reduce.\n",
    "- Client processes the entire pipeline including and after the first reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Map-Reduce Aggregates\n",
    "`SELECT aggregate(col) FROM namespace.set WHERE condition`\n",
    "\n",
    "Examples:\n",
    "- MIN\n",
    "- MAX\n",
    "- AVERAGE\n",
    "- SUM\n",
    "- COUNT\n",
    "\n",
    " `void Statement::setAggregateFunction(String udfModule, String udfFunction, ... Value udfArgs))`\n",
    " \n",
    " `ResultSet rs = Client::queryAggregate(QueryPolicy policy, Statement stmt);`\n",
    " \n",
    "Key points\n",
    "- Simple aggregates requiring a numeric state can be implemented using map and reduce.\n",
    "- The WHERE clause must be implemented using either query's index predicate or UDF's stream filter. \n",
    "\n",
    "CONFIRM:\n",
    "- Expression filters are ignored in aggregate queries?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Aggregate Operator\n",
    "Example:\n",
    "- AVERAGE\n",
    "\n",
    "Key points\n",
    "- Computing the average requires keeping track of two entities as records are processed in a stream: the sum total and the count. Average is computed at the end using the final sum and count. \n",
    "- We need to utilize aggregate operator that can hold more complex state, for example, using a map with \"sum\" and \"count\" values as the stream is processed. \n",
    "- The reducer function entails merging two  partial stream aggregates into one by adding their \"sum\" and \"count\" values. The final phase of reduce happes on the client to arrive at the final Sum and Count. \n",
    "- A map operator can then take the aggregate (map) as input and output the average value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Partitioning with GROUP BY\n",
    "\n",
    "`SELECT bin1 agg(bin2) FROM namespace.set WHERE inner-condition GROUP BY bin1`\n",
    "\n",
    "Note the inner filter \"inner-condition\" can be specified using any bins in the record, whereas the outer filter and ORDER BY must use selected (aggregated) bins from the query.\n",
    "\n",
    "Key points:\n",
    "- The aggregate state uses a map of maps. The second level maps correspond to unique grouped values in GROUP BY bin. An aggregated column value is stored within a group's map.\n",
    "- Reduce uses map-merge to merge partial aggregates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Partitions: HAVING\n",
    "`SELECT bin1 agg(bin2) FROM namespace.set WHERE inner-condition GROUP BY bin1 HAVING outer-condition`\n",
    "\n",
    "Processing for Having clause can be done using a filter operator after reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Partitions: ORDER BY\n",
    "`SELECT bin1 agg(bin2) FROM namespace.set WHERE inner-condition GROUP BY bin1 HAVING outer-condition ORDER BY bin`\n",
    "\n",
    "Processing for Order By clause can be done using a map at the end to output an ordered list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Aggregates: Distinct and Top N\n",
    "DISTINCT can be processed by storing all values in a map (as the aggregate state) that is keyed on the value(s) of the bin(s) so only unique values are retained.\n",
    "\n",
    "TOP N can be processed by retaining top N values in a list (as the aggregate state) in aggregate as well as reduce that performs list-merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways and Conclusion\n",
    "Many developers that are familiar with SQL would like to see how SQL operations translate to Aeropsike. We looked at how to implement various aggregate statements. This should be generally useful irrespective of the reader's SQL knowledge. While the examples here use synchronous execution, many operations can also be performed asynchronously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Clean up\n",
    "Remove tutorial data and close connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T20:49:19.972650Z",
     "start_time": "2020-12-29T20:49:19.967344Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed tutorial data and closed server connection.\n"
     ]
    }
   ],
   "source": [
    "client.dropIndex(null, Namespace, SmallSet, IndexName);\n",
    "client.truncate(null, Namespace, null, null);\n",
    "client.close();\n",
    "System.out.println(\"Removed tutorial data and closed server connection.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exploration and Resources\n",
    "Here are some links for further exploration\n",
    "\n",
    "Resources\n",
    "- Related notebooks\n",
    "    - [Queries](https://github.com/aerospike/aerospike-dev-notebooks.docker/blob/main/notebooks/python/query.ipynb)\n",
    "    - Other notebooks in the SQL series on 1) [SELECT](sql_select.ipynb) and 2) UPDATE, CREATE, and DELETE.\n",
    "- [Aerospike Presto Connector](https://www.aerospike.com/docs/connect/access/presto/index.html)\n",
    "- Blog post\n",
    "    - [Introducing Aerospike JDBC Connector](https://medium.com/aerospike-developer-blog/introducing-aerospike-jdbc-driver-fe46d9fc3b4d)\n",
    "- Aerospike Developer Hub\n",
    "    - [Java Developers Resources](https://developer.aerospike.com/java-developers)\n",
    "- Github repos\n",
    "    - [Java code examples](https://github.com/aerospike/aerospike-client-java/tree/master/examples/src/com/aerospike/examples)\n",
    "- Documentation\n",
    "    - [Java Client](https://www.aerospike.com/docs/client/java/index.html)\n",
    "    - [Java API Reference](https://www.aerospike.com/apidocs/java/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Visit [Aerospike notebooks repo](https://github.com/aerospike-examples/interactive-notebooks) to run additional Aerospike notebooks. To run a different notebook, download the notebook from the repo to your local machine, and then click on File->Open, and select Upload."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.8+10-LTS"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
